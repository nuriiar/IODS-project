---
title: "chapter4.Rmd"
author: "nuriiar"
date: "`r Sys.Date()`"
output: html_document
---

```{r, echo=FALSE}

```

##Exlore and load the data
#Here I am unpacking all libraries that are in use, and exploring the data. This dataset contains information collected by the U.S #Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive #(http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. However, #these comparisons were primarily done outside of Delve and are thus somewhat suspect. The dataset is small in size with only 506 #cases and 14 variables
```{r}
library(MASS)
library(tidyr)
library(corrplot)
library(ggplot2)
library(GGally)
#install.packages("GGally")
library(GGally)
data("Boston")
str(Boston)
dim(Boston)
summary(Boston)
```
##exploring the data with graphics
```{r}
Boston_pairs <- pairs(Boston)
cor_matrix <- cor(Boston)
cor_matrix
corrplot(cor_matrix, method="circle")
```
#Correlation coefficient varies between [-1; 1], with 0 meaning no correlation and 1 or -1 pointing to absolute correlation. For #example variable "rad" (index of accessibility to radial highways) is highly correlated with variable "tax" (full-value property-tax #rate per \$10,000) - with 0.91 value, which is understandable since the proximity to highways may deternime the price for a property #and hence its tax.
#Corrplot is just helping to see the strong correlations with coloured and differently sized circles for better representation of #previous results.

## Scaling the dataset and categorising the crime rate variable (crim)
```{r}

boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
bins <- quantile(boston_scaled$crim)
label_cr <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = label_cr)
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```
#Here we scaled the dataset for better comparability. ALso replaced crim variable with crime that is categorised on the basis of its #quantile points for low, medium low, medium high and high crime rates in the area. Such categorisations may reduce the data and lose #some information, yet it might be helpful for interpretation purposes.

##Dividing the dataset on train and test sets
```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
```
#We split the data to test and train sets, where the train data consists of 80% of cases chosen randomly. Like this we will be able to test the model fit.

##Fitting the linear discriminant analysis on the train set
```{r}
colnames(boston_scaled)
lda.fit <- lda(crime  ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat + medv, data = train)

lda.fit

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

# if we see the results the proportion of trace, we can see that the first dimension already achieved 95% of separation, and the 2nd #has only 0.03%, which means we the 1 dimension should be retained and analysed as the one explaining most of variation.
#For Coeffitiencs of linear discriminant of the LD1, we see that rad load the most, so it is most proximate to the crime variable.
## How our LDA model performs
```{r}
lda.pred <- predict(lda.fit, newdata = test)
help(lda.pred)
table(correct = correct_classes, predicted = lda.pred$class)

```
#What we did is we run the LDA on the train and on test data and compared the results in a crosstab. Table shows that the cases are #mostly positioned in the same groups, which shows a relatively good fit

##K-means clustering
```{r}
data1 <- data(Boston)
data1 <- scale(Boston)
data1 <- as.data.frame(data1)
class(data1)
distance <- dist(data1, method = "manhattan")
summary(distance)
km <- kmeans(data1, centers = 3)
gprs <- ggpairs(data1, col = km$cluster)
gprs
```
#





